use std::{
    array::{self, from_fn},
    borrow::{Borrow, BorrowMut},
    cmp::min,
    marker::PhantomData,
    mem,
    ops::Range,
    sync::Arc,
};

use itertools::Itertools;
use openvm_circuit::{
    arch::{
        CustomBorrow, MultiRowLayout, MultiRowMetadata, PreflightExecutor, RecordArena,
        SizedRecord, VmStateMut, *,
    },
    system::memory::{
        offline_checker::{MemoryReadAuxRecord, MemoryWriteBytesAuxRecord},
        online::TracingMemory,
        MemoryAuxColsFactory,
    },
};
use openvm_circuit_primitives::{
    bitwise_op_lookup::SharedBitwiseOperationLookupChip,
    encoder::Encoder,
    utils::{compose, next_power_of_two_or_zero},
    AlignedBytesBorrow,
};
use openvm_instructions::{
    instruction::Instruction,
    program::DEFAULT_PC_STEP,
    riscv::{RV32_CELL_BITS, RV32_MEMORY_AS, RV32_REGISTER_AS, RV32_REGISTER_NUM_LIMBS},
    LocalOpcode,
};
use openvm_rv32im_circuit::adapters::{read_rv32_register, tracing_read, tracing_write};
use openvm_sha2_air::{
    be_limbs_into_word, Sha2BlockHasherSubairConfig, Sha2DigestColsRefMut, Sha2RoundColsRefMut,
};
use openvm_stark_backend::{
    config::{StarkGenericConfig, Val},
    p3_air::{Air, AirBuilder, BaseAir},
    p3_field::{Field, FieldAlgebra, PrimeField32},
    p3_matrix::{dense::RowMajorMatrix, Matrix},
    p3_maybe_rayon::prelude::*,
    prover::{cpu::CpuBackend, types::AirProvingContext},
    rap::{BaseAirWithPublicValues, PartitionedBaseAir},
    Chip,
};

use crate::{Sha2BlockHasherChip, Sha2Config, Sha2Metadata, Sha2RecordLayout, Sha2RecordMut};

// We don't use the record arena associated with this chip. Instead, we will use the record arena
// provided by the main chip.
impl<R, RA, SC, C: Sha2Config> Chip<R, CpuBackend<SC>> for Sha2BlockHasherChip<Val<SC>, RA, C>
where
    Self: TraceFiller<Val<SC>>,
    RA: RowMajorMatrixArena<Val<SC>>,
    Val<SC>: PrimeField32,
    SC: StarkGenericConfig,
{
    // We use the generic parameter R to allow any record arena to be passed in
    fn generate_proving_ctx(&self, _: R) -> AirProvingContext<CpuBackend<SC>> {
        // SAFETY: the tracegen for Sha2MainChip must be done before this chip's tracegen
        let arena = self.arena.lock().unwrap().take().unwrap();

        let rows_used = arena.trace_offset() / arena.width();
        let mut trace = arena.into_matrix();
        let mem_helper = self.mem_helper.as_borrowed();

        self.fill_trace(&mem_helper, &mut trace, rows_used);

        AirProvingContext::simple(Arc::new(trace), self.generate_public_values())
    }
}

impl<F, RA, C> TraceFiller<F> for Sha2BlockHasherChip<F, RA, C>
where
    RA: RowMajorMatrixArena<F> + Send + Sync,
    F: PrimeField32,
    C: Sha2BlockHasherSubairConfig,
{
    fn fill_trace(
        &self,
        _mem_helper: &MemoryAuxColsFactory<F>,
        trace_matrix: &mut RowMajorMatrix<F>,
        rows_used: usize,
    ) {
        if rows_used == 0 {
            return;
        }

        let trace = &mut trace_matrix.values[..];

        // During the first pass we will fill out most of the matrix
        // But there are some cells that can't be generated by the first pass so we will do a second
        // pass over the matrix later
        trace
            .par_chunks_mut(C::WIDTH * C::ROWS_PER_BLOCK)
            .enumerate()
            .for_each(|(block_idx, mut block_slice)| {
                if block_idx * C::ROWS_PER_BLOCK >= rows_used {
                    // this is a dummy row
                    // Fill in the invalid rows
                    block_slice.par_chunks_mut(C::WIDTH).for_each(|row| {
                        // Need to get rid of the accidental garbage data that might overflow
                        // the F's prime field. Unfortunately, there
                        // is no good way around this SAFETY:
                        // - row has exactly C::WIDTH elements
                        // - We're zeroing all C::WIDTH elements to clear any garbage data that
                        //   might overflow the field
                        // - Casting F* to u8* preserves validity for write_bytes operation
                        // - C::WIDTH * size_of::<F>() correctly calculates total bytes to zero
                        unsafe {
                            std::ptr::write_bytes(
                                row.as_mut_ptr() as *mut u8,
                                0,
                                C::WIDTH * size_of::<F>(),
                            );
                        }
                        let cols = Sha2RoundColsRefMut::from::<C>(&mut row[..C::ROUND_WIDTH]);

                        self.inner.generate_default_row(cols);
                    });
                    return;
                }

                // SAFETY:
                // - caller ensures `trace` contains a valid record representation that was
                //   previously written by the executor
                // - slice contains a valid Sha256VmRecord with the exact layout specified
                // - get_record_from_slice will correctly split the buffer into header, input, and
                //   aux components based on this layout
                let record: Sha2RecordMut = unsafe {
                    get_record_from_slice(
                        &mut block_slice,
                        Sha2RecordLayout {
                            metadata: Sha2Metadata {
                                variant: C::VARIANT,
                            },
                        },
                    )
                };

                // Need to get rid of the accidental garbage data that might overflow the
                // F's prime field. Unfortunately, there is no good way around this
                // SAFETY:
                // - block_slice comes from par_chunks_exact_mut with exact size guarantee
                // - Length is SHA256_ROWS_PER_BLOCK * SHA256VM_WIDTH * size_of::<F>() bytes
                // - Zeroing entire blocks prevents using garbage data
                // - The subsequent trace filling will overwrite with valid values
                unsafe {
                    std::ptr::write_bytes(
                        block_slice.as_mut_ptr() as *mut u8,
                        0,
                        C::ROWS_PER_BLOCK * C::WIDTH * size_of::<F>(),
                    );
                }

                let prev_hash = (0..C::HASH_WORDS)
                    .map(|i| {
                        be_limbs_into_word::<C>(
                            &record.prev_state[i * C::WORD_U8S..(i + 1) * C::WORD_U8S]
                                .iter()
                                .map(|x| *x as u32)
                                .collect::<Vec<_>>(),
                        )
                    })
                    .collect::<Vec<_>>();

                self.fill_block_trace(
                    block_slice,
                    &record.message_bytes[block_idx * C::BLOCK_U8S..(block_idx + 1) * C::BLOCK_U8S],
                    true,
                    block_idx,
                    &prev_hash,
                );
            });

        // Do a second pass over the trace to fill in the missing values
        // Note, we need to skip the very first row
        trace[C::WIDTH..]
            .par_chunks_mut(C::WIDTH * C::ROWS_PER_BLOCK)
            .take(rows_used / C::ROWS_PER_BLOCK)
            .for_each(|chunk| {
                self.inner.generate_missing_cells(chunk, C::WIDTH);
            });
    }
}

impl<F, RA, C: Sha2BlockHasherSubairConfig> Sha2BlockHasherChip<F, RA, C> {
    #[allow(clippy::too_many_arguments)]
    fn fill_block_trace(
        &self,
        block_slice: &mut [F],
        input: &[u8],
        is_last_block: bool,
        global_block_idx: usize,
        prev_hash: &[C::Word],
    ) where
        F: PrimeField32,
    {
        debug_assert_eq!(input.len(), C::BLOCK_U8S);
        debug_assert_eq!(prev_hash.len(), C::HASH_WORDS);

        // Fill in the VM columns first because the inner `carry_or_buffer` needs to be filled in
        block_slice
            .par_chunks_exact_mut(C::WIDTH)
            .enumerate()
            .for_each(|(row_idx, row_slice)| {
                // Handle round rows and digest row separately
                if row_idx == C::ROWS_PER_BLOCK - 1 {
                    // This is a digest row
                    let digest_cols = Sha2DigestColsRefMut::<F>::from::<C>(
                        row_slice[..C::DIGEST_WIDTH].borrow_mut(),
                    );
                    *digest_cols.flags.is_last_block = F::from_bool(is_last_block);
                    *digest_cols.flags.is_digest_row = F::from_bool(true);
                } else {
                    // This is a round row
                    let mut round_cols = Sha2RoundColsRefMut::<F>::from::<C>(
                        row_slice[..C::ROUND_WIDTH].borrow_mut(),
                    );
                    // Take care of the first 4 round rows (aka read rows)
                    if row_idx < C::MESSAGE_ROWS {
                        round_cols
                            .message_schedule
                            .carry_or_buffer
                            .iter_mut()
                            .zip(
                                input[row_idx * C::ROUNDS_PER_ROW * C::WORD_U8S
                                    ..(row_idx + 1) * C::ROUNDS_PER_ROW * C::WORD_U8S]
                                    .iter(),
                            )
                            .for_each(|(cell, data)| {
                                *cell = F::from_canonical_u8(*data);
                            });
                    }
                }
            });

        let input = (0..C::BLOCK_WORDS)
            .map(|i| {
                be_limbs_into_word::<C>(
                    &input[i * C::WORD_U8S..(i + 1) * C::WORD_U8S]
                        .iter()
                        .map(|x| *x as u32)
                        .collect::<Vec<_>>(),
                )
            })
            .collect::<Vec<_>>();

        // Fill in the inner trace when the `carry_or_buffer` is filled in
        self.inner.generate_block_trace(
            block_slice,
            C::WIDTH,
            &input,
            self.bitwise_lookup_chip.clone(),
            prev_hash,
            is_last_block,
            global_block_idx as u32 + 1, // global block index is 1-indexed
        );
    }
}
